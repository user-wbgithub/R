## 判别分析（分类）

#### 聚类和分类的区别

分类：是以知有多少类，在训练样本下，利用训练样本得到判别函数，对待测样本进行分类。

聚类：是不知道有多少类，根据某种规则讲样本进行分类。

判别分析是监督学习

聚类是无监督学习

#### 常见的分类模型与算法

线性判别法

距离判别法(KNN)

贝叶斯分类器

决策树

支持向量机(SVM)

某些神经网络

## 线性判别法Fisher判别

```R
> data<-read.table("fisher.txt",header=T)
> data
   num    X1   X2 Y
1    1  -1.9  3.2 1
2    2  -6.9  0.4 1
3    3   5.2  2.0 1
4    4   5.0  2.5 1
5    5   7.3  0.0 1
6    6   6.8 12.7 1
7    7   0.9 -5.4 1
8    8 -12.5 -2.5 1
9    9   1.5  1.3 1
10  10   3.8  6.8 1
11  11   0.2  6.2 2
12  12  -0.1  7.5 2
13  13   0.4 14.6 2
14  14   2.7  8.3 2
15  15   2.1  0.8 2
16  16  -4.6  4.3 2
17  17  -1.7 10.9 2
18  18  -2.6 13.1 2
19  19   2.6 12.8 2
20  20  -2.8 10.0 2
> attach(data)
> plot(X1,X2)
> text(X1,X2,Y,adj=--0.5)# adj=-0,5表示标注和点的距离
```

##### 画出数据的图像并且标注：

![](https://github.com/user-wbgithub/R/blob/master/images/2020-02-09_234538.jpg)

![](https://github.com/user-wbgithub/R/blob/master/images/2020-02-09_234818.jpg)

上图可以清洗的找到一条线讲数据集分为上下两部分，上部分为类别2，下部分为类别1。

在R中MASS包可以做线性判别

##### 导入MASS包-->建立模型(lda)

```R
> library(MASS)
> model<-lda(Y~X1+X2)
> model
Call:
lda(Y ~ X1 + X2)

Prior probabilities of groups:
  1   2 
0.5 0.5 

Group means:
     X1   X2
1  0.92 2.10
2 -0.38 8.85

Coefficients of linear discriminants:
          LD1
X1 -0.1035305
X2  0.2247957

```

##### 结果分析：

Prior probabilities of groups:

指的是每个类所占的百分比，本例是各占50%

Coefficients of linear discriminants:

指的是所求出的系数，本类中X1 -0.1035305、X2  0.2247957

##### 使用模型做预测（predict）

```
> p=predict(model)
> p
$class
 [1] 1 1 1 1 1 2 1 1 1 1 2 2 2 2 1 2 2 2 2 2
Levels: 1 2

$posterior
            1          2
1  0.61625867 0.38374133
2  0.65888888 0.34111112
3  0.89412853 0.10587147
4  0.87143887 0.12856113
5  0.96214822 0.03785178
6  0.17275662 0.82724338
7  0.98442237 0.01557763
8  0.68514398 0.31485602
9  0.85330562 0.14669438
10 0.52789262 0.47210738
11 0.43015877 0.56984123
12 0.30676827 0.69323173
13 0.03336323 0.96663677
14 0.34672296 0.65327704
15 0.88585263 0.11414737
16 0.40213732 0.59786268
17 0.08694507 0.91305493
18 0.03480991 0.96519009
19 0.08934413 0.91065587
20 0.09926372 0.90073628

$x
           LD1
1  -0.28674901
2  -0.39852439
3  -1.29157053
4  -1.15846657
5  -1.95857603
6   0.94809469
7  -2.50987753
8  -0.47066104
9  -1.06586461
10 -0.06760842
11  0.17022402
12  0.49351760
13  2.03780185
14  0.38346871
15 -1.24038077
16  0.24005867
17  1.42347182
18  2.01119984
19  1.40540244
20  1.33503926
```

| 原分类结果   | 1 1 1 1 1 1 1 1 1 1 2 2 2 2 2 2 2 2 2  |
| ------------ | -------------------------------------- |
| 模型分类结果 | 1  1 1 1 1 2 1 1 1 1 2 2 2 2 1 2 2 2 2 |

模型判断率很高，仅有2个样本点误差。

## KNN最近邻判别法

当数据分布不是那么规律时就不能用Fisher判别了

![](https://github.com/user-wbgithub/R/blob/master/images/014.png)

KNN是分类算法中最简单的方法之一，是指最近的K个邻居可以代表整体样本。

**KNN的核心思想：如果一个样本在特征空间中的k个最相邻的样本中的大多数属于某个类别，则该样本也属于这个类别，具有这个类别上样本的特性**。

#### KNN算法原理

![](https://github.com/user-wbgithub/R/blob/master/images/020.png)

如要找黑色点属于哪个类，会找到黑色点附近的随机K个点，K通常是奇数。然后计算样本点到每个点的距离，比较那类点的个数多就是那个类

**使用R自带的数据集iris-->导入class包-->建立模型knn.cv（变量，编号，分类数，判别概率）-->查看模型-->查看模型概要信息summary**

```R
> library(class)
> model.knn<-knn.cv(iris[,1:4],iris[,5],k=3,prob=TRUE)
> model.knn
  [1] setosa     setosa     setosa     setosa     setosa     setosa    
  [7] setosa     setosa     setosa     setosa     setosa     setosa    
 [13] setosa     setosa     setosa     setosa     setosa     setosa    
 [19] setosa     setosa     setosa     setosa     setosa     setosa    
 [25] setosa     setosa     setosa     setosa     setosa     setosa    
 [31] setosa     setosa     setosa     setosa     setosa     setosa    
 [37] setosa     setosa     setosa     setosa     setosa     setosa    
 [43] setosa     setosa     setosa     setosa     setosa     setosa    
 [49] setosa     setosa     versicolor versicolor versicolor versicolor
 [55] versicolor versicolor versicolor versicolor versicolor versicolor
 [61] versicolor versicolor versicolor versicolor versicolor versicolor
 [67] versicolor versicolor versicolor versicolor virginica  versicolor
 [73] virginica  versicolor versicolor versicolor versicolor versicolor
 [79] versicolor versicolor versicolor versicolor versicolor virginica 
 [85] versicolor versicolor versicolor versicolor versicolor versicolor
 [91] versicolor versicolor versicolor versicolor versicolor versicolor
 [97] versicolor versicolor versicolor versicolor virginica  virginica 
[103] virginica  virginica  virginica  virginica  versicolor virginica 
[109] virginica  virginica  virginica  virginica  virginica  virginica 
[115] virginica  virginica  virginica  virginica  virginica  versicolor
[121] virginica  virginica  virginica  virginica  virginica  virginica 
[127] virginica  virginica  virginica  virginica  virginica  virginica 
[133] virginica  versicolor virginica  virginica  virginica  virginica 
[139] virginica  virginica  virginica  virginica  virginica  virginica 
[145] virginica  virginica  virginica  virginica  virginica  virginica 
attr(,"prob")
  [1] 1.0000000 1.0000000 1.0000000 1.0000000 1.0000000 1.0000000 1.0000000
  [8] 1.0000000 1.0000000 1.0000000 1.0000000 1.0000000 1.0000000 1.0000000
 [15] 1.0000000 1.0000000 1.0000000 1.0000000 1.0000000 1.0000000 1.0000000
 [22] 1.0000000 1.0000000 1.0000000 1.0000000 1.0000000 1.0000000 1.0000000
 [29] 1.0000000 1.0000000 1.0000000 1.0000000 1.0000000 1.0000000 1.0000000
 [36] 1.0000000 1.0000000 1.0000000 1.0000000 1.0000000 1.0000000 1.0000000
 [43] 1.0000000 1.0000000 1.0000000 1.0000000 1.0000000 1.0000000 1.0000000
 [50] 1.0000000 1.0000000 1.0000000 1.0000000 1.0000000 1.0000000 1.0000000
 [57] 1.0000000 1.0000000 1.0000000 1.0000000 1.0000000 1.0000000 1.0000000
 [64] 1.0000000 1.0000000 1.0000000 1.0000000 1.0000000 0.6666667 1.0000000
 [71] 1.0000000 1.0000000 1.0000000 1.0000000 1.0000000 1.0000000 1.0000000
 [78] 0.6666667 1.0000000 1.0000000 1.0000000 1.0000000 1.0000000 1.0000000
 [85] 1.0000000 1.0000000 1.0000000 1.0000000 1.0000000 1.0000000 1.0000000
 [92] 1.0000000 1.0000000 1.0000000 1.0000000 1.0000000 1.0000000 1.0000000
 [99] 1.0000000 1.0000000 1.0000000 1.0000000 1.0000000 1.0000000 1.0000000
[106] 1.0000000 1.0000000 1.0000000 1.0000000 1.0000000 0.7500000 1.0000000
[113] 1.0000000 1.0000000 1.0000000 1.0000000 1.0000000 1.0000000 1.0000000
[120] 1.0000000 1.0000000 1.0000000 1.0000000 0.7500000 1.0000000 1.0000000
[127] 1.0000000 1.0000000 1.0000000 1.0000000 1.0000000 1.0000000 1.0000000
[134] 0.6666667 0.6666667 1.0000000 1.0000000 1.0000000 0.6666667 1.0000000
[141] 1.0000000 1.0000000 1.0000000 1.0000000 1.0000000 1.0000000 1.0000000
[148] 1.0000000 1.0000000 1.0000000
Levels: setosa versicolor virginica
> summary(model.knn)
    setosa versicolor  virginica 
        50         50         50 

```

模型存在5个与原样本不一致的误判，精准度97%。

## VSM支持向量机

1.线性分类器

2.有约束优化模型（岭回归）

3.特征空间构造

4.核函数

5.拉格朗日定理

#### 特征空间构造

在原空间直接求解可能会很复杂、甚至无法求解。所以可以映射到摸一个特征空间来求解，使问题变得简单

![](https://github.com/user-wbgithub/R/blob/master/images/1523.jpg)

![](https://github.com/user-wbgithub/R/blob/master/images/1524.jpg)

在解决分类问题时，在低维空间是非线性可分时，可以采用升维的特征空间构造手段，在高位的特征空间里，找到合适的线性分类器。从而变得线性可分。

即是：在高维特征空间，解决线性可分问题。

之所以追求线性可分，是因为非线性分类器难求。

#### 核函数

一般算法不采用升维的方式解决分类，因为维度越高计算代价越大。会带来位数灾难问题。

SVM采用升维的方式解决分类，因为底层还使用了核函数机制。

核函数：在某一低维空间中找到一个和函数，使用这个核函数计算高维的特征空间数据。

以下例子：

![](https://github.com/user-wbgithub/R/blob/master/images/2020-02-11_153209.jpg)

#### 拉格朗日定理

拉格朗日定理用于求解有约束的最优化问题

需要符合条件：

1.求解最优化问题

2.有一个目标函数f(β)

3.有一个约束函数h(β)

例子

![](https://github.com/user-wbgithub/R/blob/master/images/2020-02-11_154014.jpg)

考虑盒子的三个变长w，u，v。规定盒子的表面积为定值c。求体积最小

![](https://github.com/user-wbgithub/R/blob/master/images/2020-02-11_164536.jpg)

#### SVM Suport Vector Machine

支持向量机，用于解决高维度数据的分类，如图片分类等场景