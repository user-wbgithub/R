### 建立多元线性回归模型

**多元线性与一元线性相比自变量变为多个而不是一个，具体建模如下**：

建立数据集(数据框)-->建立模型(lm)-->检测模型(summary)-->使用模型做预测(predict)

```R
> blood<-data.frame(
+ X1=c(76.0, 91.5, 85.5, 82.5, 79.0, 80.5, 74.5,
+ 79.0, 85.0, 76.5, 82.0, 95.0, 92.5),
+ X2=c(50, 20, 20, 30, 30, 50, 60, 50, 40, 55,
+ 40, 40, 20),
+ Y= c(120, 141, 124, 126, 117, 125, 123, 125,132, 123, 132, 155, 147))

> lm.sol<-lm(Y ~ X1+X2, data=blood)

> blood
     X1 X2   Y
1  76.0 50 120
2  91.5 20 141
3  85.5 20 124
4  82.5 30 126
5  79.0 30 117
6  80.5 50 125
7  74.5 60 123
8  79.0 50 125
9  85.0 40 132
10 76.5 55 123
11 82.0 40 132
12 95.0 40 155
13 92.5 20 147
>

> summary(lm.sol) 

Call:
lm(formula = Y ~ X1 + X2, data = blood)

Residuals:
    Min      1Q  Median      3Q     Max 
-4.0404 -1.0183  0.4640  0.6908  4.3274 

Coefficients:
             Estimate Std. Error t value Pr(>|t|)    
(Intercept) -62.96336   16.99976  -3.704 0.004083 ** 
X1            2.13656    0.17534  12.185 2.53e-07 ***
X2            0.40022    0.08321   4.810 0.000713 ***
---
Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1

Residual standard error: 2.854 on 10 degrees of freedom
Multiple R-squared:  0.9461,    Adjusted R-squared:  0.9354 
F-statistic: 87.84 on 2 and 10 DF,  p-value: 4.531e-07

> new <- data.frame(X1 = 80, X2 = 40)
> lm.pred<-predict(lm.sol, new, interval="prediction", level=0.95)
> lm.pred
       fit      lwr      upr
1 123.9699 117.2889 130.6509
```



### 多重共线性

#### 	完全共线性

​		回归模型自变量之间存在精确相关关系或者高度相关关系，使得模型失真。	

```R
> toothpaste<-read.table("toothpaste.txt",header=T)
> toothpaste
     X1   X2    X3   X4    Y
1  3.85 3.80 -0.05 5.50 7.38
2  3.75 4.00  0.25 6.75 8.51
3  3.70 4.30  0.60 7.25 9.52
4  3.70 3.70  0.00 5.50 7.50
5  3.60 3.85  0.25 7.00 9.33
> model<-lm(Y~X1+X2+X3+X4,data=toothpaste)
>
> summary(model)

Call:
lm(formula = Y ~ X1 + X2 + X3 + X4, data = toothpaste)

Residuals:
     Min       1Q   Median       3Q      Max 
-0.41148 -0.11647 -0.01044  0.13584  0.51221 

Coefficients: (1 not defined because of singularities)
            Estimate Std. Error t value Pr(>|t|)    
(Intercept)   7.5227     2.4450   3.077 0.004881 ** 
X1           -2.3352     0.6388  -3.655 0.001141 ** 
X2            1.6006     0.2961   5.406 1.15e-05 ***
X3                NA         NA      NA       NA    
X4            0.5057     0.1261   4.009 0.000457 ***
---
Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1

Residual standard error: 0.2341 on 26 degrees of freedom
Multiple R-squared:  0.8942,    Adjusted R-squared:  0.882 
F-statistic: 73.22 on 3 and 26 DF,  p-value: 8.307e-13
```

​		*其中多元R方值很高，模型拟合很好。但是X3为NA。其中X3=X2-X1存在精确的函数关系，数据存在**完全共线性***

#### 	近似共线性

​		回归模型自变量之间存在一定程度上的共线性（多见）

#### 	共线性的解决办法

##### 		解决方法

###### 		1.手动解决解决

​			*手动剔除具有相关性的自变量*

```R
> model<-lm(Y~X1+X2+X4,data=toothpaste)
> summary(model)

Call:
lm(formula = Y ~ X1 + X2 + X4, data = toothpaste)

Residuals:
     Min       1Q   Median       3Q      Max 
-0.41148 -0.11647 -0.01044  0.13584  0.51221 

Coefficients:
            Estimate Std. Error t value Pr(>|t|)    
(Intercept)   7.5227     2.4450   3.077 0.004881 ** 
X1           -2.3352     0.6388  -3.655 0.001141 ** 
X2            1.6006     0.2961   5.406 1.15e-05 ***
X4            0.5057     0.1261   4.009 0.000457 ***
---
Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1

Residual standard error: 0.2341 on 26 degrees of freedom
Multiple R-squared:  0.8942,    Adjusted R-squared:  0.882 
F-statistic: 73.22 on 3 and 26 DF,  p-value: 8.307e-13
```

​			*手动剔除共线性自变量当自变量很多时，会尝试很多次，所以不适用于生产*。

###### 		2.逐步回归方法解决

```
> mode<-lm(Y~X1+X2+X3+X4,data=data)
> summary(model)

Call:
lm(formula = Y ~ X1 + X2 + X3 + X4, data = data)

Residuals:
    Min      1Q  Median      3Q     Max 
-3.1750 -1.6709  0.2508  1.3783  3.9254 

Coefficients:
            Estimate Std. Error t value Pr(>|t|)  
(Intercept)  62.4054    70.0710   0.891   0.3991  
X1            1.5511     0.7448   2.083   0.0708 .
X2            0.5102     0.7238   0.705   0.5009  
X3            0.1019     0.7547   0.135   0.8959  
X4           -0.1441     0.7091  -0.203   0.8441  
---
Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1

Residual standard error: 2.446 on 8 degrees of freedom
Multiple R-squared:  0.9824,    Adjusted R-squared:  0.9736 
F-statistic: 111.5 on 4 and 8 DF,  p-value: 4.756e-07
```

​			*①这里R方值很高，模型拟合度很好，但是所有的系数都没有通过检验。Pr都大于0.05。这里通过逐步回归方法剔除变量。*

​			*②逐步回归方法step()评估指标是AIC，AIC越小说明效果越好。底层使每次剔除一个自变量*。

```R
> model<-lm(Y~X1+X2+X3+X4,data=data)
> model.step<-step(model)
Start:  AIC=26.94
Y ~ X1 + X2 + X3 + X4

       Df Sum of Sq    RSS    AIC
- X3    1    0.1091 47.973 24.974
- X4    1    0.2470 48.111 25.011
- X2    1    2.9725 50.836 25.728
<none>              47.864 26.944
- X1    1   25.9509 73.815 30.576

Step:  AIC=24.97
Y ~ X1 + X2 + X4

       Df Sum of Sq    RSS    AIC
<none>               47.97 24.974
- X4    1      9.93  57.90 25.420
- X2    1     26.79  74.76 28.742
- X1    1    820.91 868.88 60.629
```

​			*①模型调用step()进行逐步回归，根据AIC从大到小进行剔除变量，最小的效果最好。上面代码第一次去除X3后效果最好，第二次什么都不剔除效果最好。查看剔除后的模型：*

```R
> summary(model.step)

Call:
lm(formula = Y ~ X1 + X2 + X4, data = data)

Residuals:
    Min      1Q  Median      3Q     Max 
-3.0919 -1.8016  0.2562  1.2818  3.8982 

Coefficients:
            Estimate Std. Error t value Pr(>|t|)    
(Intercept)  71.6483    14.1424   5.066 0.000675 ***
X1            1.4519     0.1170  12.410 5.78e-07 ***
X2            0.4161     0.1856   2.242 0.051687 .  
X4           -0.2365     0.1733  -1.365 0.205395    
---
Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1

Residual standard error: 2.309 on 9 degrees of freedom
Multiple R-squared:  0.9823,    Adjusted R-squared:  0.9764 
F-statistic: 166.8 on 3 and 9 DF,  p-value: 3.323e-08
```

​			*①.模型拟合度很高，但是有两个系数没有通过检验。通过逐步回归方法的辅助，选择AIC最小的X4进行进一步剔除。查看模型：*

```

> model<-lm(Y~X1+X2,data=data)
> summary(model)

Call:
lm(formula = Y ~ X1 + X2, data = data)

Residuals:
   Min     1Q Median     3Q    Max 
-2.893 -1.574 -1.302  1.363  4.048 

Coefficients:
            Estimate Std. Error t value Pr(>|t|)    
(Intercept) 52.57735    2.28617   23.00 5.46e-10 ***
X1           1.46831    0.12130   12.11 2.69e-07 ***
X2           0.66225    0.04585   14.44 5.03e-08 ***
---
Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1

Residual standard error: 2.406 on 10 degrees of freedom
Multiple R-squared:  0.9787,    Adjusted R-squared:  0.9744 
F-statistic: 229.5 on 2 and 10 DF,  p-value: 4.407e-09
```

​			*①.这里模型R方值很高，模型拟合很好，所有系数也都通过检验*

​			*②.以上就是使用逐步回归方法进行模型的优化，适用于生产*

###### 		3.岭回归算法解决

​			先导入岭回归的程序包-->导入ridge包-->数据框数据集-->建立回归模型(linearRidge)-->画岭际图(MASS lm.ridge)

```R
> utils:::menuInstallLocal()
程序包‘ridge’打开成功，MD5和检查也通过
> library(ridge)
> data<-read.table("cement.txt",header=T)
> data
   X1 X2 X3 X4     Y
1   7 26  6 60  78.5
2   1 29 15 52  74.3
3  11 56  8 20 104.3
4  11 31  8 47  87.6
5   7 52  6 33  95.9
6  11 55  9 22 109.2
7   3 71 17  6 102.7
8   1 31 22 44  72.5
9   2 54 18 22  93.1
10 21 47  4 26 115.9
11  1 40 23 34  83.8
12 11 66  9 12 113.3
13 10 68  8 12 109.4
> model<-lm(Y~X1+X2+X3+X4,data=data)
> summary(model)

Call:
lm(formula = Y ~ X1 + X2 + X3 + X4, data = data)

Residuals:
    Min      1Q  Median      3Q     Max 
-3.1750 -1.6709  0.2508  1.3783  3.9254 

Coefficients:
            Estimate Std. Error t value Pr(>|t|)  
(Intercept)  62.4054    70.0710   0.891   0.3991  
X1            1.5511     0.7448   2.083   0.0708 .
X2            0.5102     0.7238   0.705   0.5009  
X3            0.1019     0.7547   0.135   0.8959  
X4           -0.1441     0.7091  -0.203   0.8441  
---
Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1

Residual standard error: 2.446 on 8 degrees of freedom
Multiple R-squared:  0.9824,    Adjusted R-squared:  0.9736 
F-statistic: 111.5 on 4 and 8 DF,  p-value: 4.756e-07
```

​			这里使用最小二乘法建立的多元线性模型的拟合度很高，但是系数都没有通过检验，我们再建	立逻辑回归模型。

```R
> model.ridge<-linearRidge(Y~X1+X2+X3+X4,data=data)
> summary(model.ridge)

Call:
linearRidge(formula = Y ~ X1 + X2 + X3 + X4, data = data)


Coefficients:
            Estimate Scaled estimate Std. Error (scaled) t value (scaled)
(Intercept)  83.7040              NA                  NA               NA
X1            1.2922         26.3321              3.6721            7.171
X2            0.2977         16.0463              3.9883            4.023
X3           -0.1478         -3.2790              3.5979            0.911
X4           -0.3506        -20.3290              3.9963            5.087
            Pr(>|t|)    
(Intercept)       NA    
X1          7.45e-13 ***
X2          5.74e-05 ***
X3             0.362    
X4          3.64e-07 ***
---
Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1

Ridge parameter: 0.01473162, chosen automatically, computed using 2 PCs

Degrees of freedom: model 3.01 , variance 2.837 , residual 3.183 
```

​			使用岭回归算法之后X1、X2、X4系数都通过检验，但是X3系数未通过检验，岭回归模型并未对X3自变量进行剔除，这是岭回归的一个缺点，很难剔除共线性的列，达到筛选的效果。原因如下图：

![](https://github.com/user-wbgithub/R/blob/master/images/4.png)

​			假如要剔除β1必须是：最小二乘解和约束方程必须相交于β2轴上才可以，这非常难，所以岭回归算法很难剔除共线性列

```R
> library(MASS)
> plot(lm.ridge(data$Y~.,data,lambda=seq(0,150,length=151)))
```

​			画出岭际图：lambda表达式生成的是零参数的区间

![](https://github.com/user-wbgithub/R/blob/master/images/2020-02-06_160935.jpg)

### 算法

#### 	最小二乘法

​		最小二乘法有个使用条件：样本矩阵必须是满秩的。(数据集不能出现完全共线性的情况)

​		满秩---矩阵的所有行/列都不为0；

​		转秩---行变列裂变行T

##### 	多重共线性和最小二乘法

​		最小二乘法的限制：**样本矩阵的行列式不能为0**，样本矩阵必须是非奇异矩阵（满秩）。样本矩阵		行列式不为零，样本特征表现为：**个个特征之间不能表现为强烈线性相关性（多重共线性）**，线		性相关性经过初等变化会出现某行/列为0。

​		*假设y是因变量矩阵；β是系数矩阵；X是样本矩阵；ε是误差矩阵；*
$$
y = \left[
\matrix{
  y_1 & \\
  y_2 & \\
  ⋮ & \\
  y_3 
}
\right],β=\left[
\matrix{
  β_1 & \\
  β_2 & \\
  ⋮ & \\
  β_3 
}
\right],X=\left[
\matrix{
  1 & x_{11} &x_{12} &...&x_{1p}\\
  1 & x_{21} &x_{22} &...&x_{2p}\\
  ⋮ & ⋮&⋮&&⋮\\
  1 & x_{n1} & x_{2n} & ... & x_{np}
}
\right],ε=\left[
\matrix{
  ε_1 & \\
  ε_2 & \\
  ⋮ & \\
  ε_3 
}
\right]
$$
​		*多元线性模型可以表示为：*

​			
$$
Y=Xβ+ε
$$
​		*经过推导求得的系数最小二乘解可表示为如下形式：*

​	
$$
β=(X^TX)^{-1}X^TY
$$
​		*系数矩阵等于：X矩阵的转秩乘以X矩阵还是X矩阵，再求出这个整体的逆矩阵，再乘以X矩阵的转		秩和Y因变量矩阵。*

​		*当出现多重共线性时矩阵中某行/列为0：*
$$
A = \left[\matrix{1&2\\ 2&4\\ 3&6}\right]=\left[\matrix{2&2\\ 4&4\\ 6&6}\right]=\left[\matrix{0&2\\ 0&4\\ 0&6}\right]
$$

​		*计算A矩阵的转秩和A相乘*
$$
\left[\matrix{A^TA\\ }\right]=\left[\matrix{ 0&0&0\\ 2&4&6\\ }\right]=\left[\matrix{ 0&0\\ 0&56\\ } \right] =0\ast56-0\ast0=0
$$
​		*①此时计算出来的行列式矩阵是0，这个矩阵的逆所以当样本矩阵中存在多重共线性（完全）问题时，系数矩阵是求不出来的。所以最小二乘法不适用于样本矩阵不满秩的情况。*

​		*②当样本矩阵满秩，但是具有强烈**近似共线性**时，所求出来的系数矩阵很大，d模型失去解释能力；**最小二乘法适用场景：当样本矩阵的共线性程度较弱时，可以使用。***

​	*③强烈近似共线时，可以使用岭回归进行有偏估计，最小二乘法是一种无偏差估计。*

#### 岭回归算法

​		岭回归是**用于共线性数据分析的有偏估计回归方法，**在最小二乘法上进行了改良--**增加了惩戒系数**。放弃了最小二乘法的无偏性，以损失精度的方式获得回归系数更符合实际。对于病态数据的拟合要更优于最小二乘法。

​		最小二乘法：
$$
β=(X^TX)^{-1}X^TY
$$
​		岭回归：
$$
β=(X^TX+λI)^{-1}X^TY
$$
​		λ：岭参数大于0；Ⅰ：与样本矩阵同维度的单位矩阵；

​		①岭回归在最小二乘解上增加了一个小的扰动（惩罚措施），整体上要比最小二乘的估计值更稳定（在样本矩阵有严重共线性时）；

​		②岭回归是有偏估计的原因：在原数据的基础上引入了其他数据；

​		岭回归核心思想：寻找最岭参数λ，在这个零参数之下，使得残差平方和最小，从而求解出岭回归系数。
$$
β=(X^TX+λI)^{-1}X^TY
$$

$$
RSS(λ)=(y-Xβ)^T(y-Xβ)+λβ^Tβ\tag1
$$

​		(1)式等价于在λ作用下，求解系数β，使得RSS最小:
$$
\hat{β}^{ridge}=argmin\left\lbrace \sum_{i=1}^{N}(y_i-\beta_0-\sum_{j=1}^{p}x_{ij}\beta_j)^2+\lambda\sum_{j=1}^p\beta^2_j\right\rbrace\tag2
$$
​		进一步将(2)式推导，转变为就系数βj的约束边界问题:
$$
\hat{β}^{ridge}=argmin\left\lbrace \sum_{i=1}^{N}(y_i-\beta_0-\sum_{j=1}^{p}x_{ij}\beta_j)^2\right\rbrace，
$$

$$
subject\quad to\quad\sum_{j=i}^p\beta^2_j\leq t,\tag3
$$

​		边界约束的解释：

​		例如有两个自变量身高X1和体重X2，因变量Y式某种激素水平，建立的模型为：
$$
Y=\beta0+\beta1X1+\beta2X2
$$
​		在公式(3)中的约束为：
$$
\beta1^2+\beta2^2 ≦t
$$
​		①直接使用最小二乘法去拟合得到唯一解，但是X1和X2严重相关，可能会出现β1和β2相互抵消的效应，如β1是一个很大的数，同时β2是一个绝对值很大负数。虽然使得Y的变换很稳定，但是模型已经失真。

​		②转变为使用岭回归法加上一个约束，将β1和β2固定在一个区间之内。

#### 岭回归算法的几何解释

​		以上面边界约束中的例子为例，求解一组β1和β2，使得RSS最小，并受限于约束公式为：
$$
\beta1^2+\beta2^2 ≦t
$$
​		求解两个系数为：

![。](https://github.com/user-wbgithub/R/blob/master/images/2020-02-06_144210.jpg)

​		凸图形最低点时最小二乘解，岭回归解是：约束方程和最小二乘解的焦点处对应的坐标解。具体可以用拉格朗日定理求解，解决有参数化的问题。

#### 岭际图

​		岭际图描述的是零参数k变化时，系数值的变化。岭际图呈喇叭口形状，则可以认为数据存在对冲共线性，张口越大共线性越强

![1](https://github.com/user-wbgithub/R/blob/master/images/1.png)

​		λ等于0时，系数值较大，但当λ增大系数值迅速缩小，说明引入零参数之后有很强的限制效果，说明有严重的共线性

![2](https://github.com/user-wbgithub/R/blob/master/images/2.png)

​		这个岭际图也反映出多个变量之间存在共线性问题

![3](https://github.com/user-wbgithub/R/blob/master/images/3.png)

​		这个岭际图比较平稳，最小二乘法可以有更大信心

#### LASSO算法

​	逐步回归法可以筛选变量，但是当自变量很多时计算量会很大，因为他是类似于排列组合的；

​	岭回归算法添加约束可以解决严重近似共线性，但是不能剔除严重共线性的自变量列；

​	LASSO算法既可以解决共线性，也擅长筛选自变量；

​	但是一般使用LAR算法，因为LASSO算法和LAR算法解出的自变量非常接近。

岭回归：
$$
\hat{β}^{ridge}=argmin\left\lbrace \sum_{i=1}^{N}(y_i-\beta_0-\sum_{j=1}^{p}x_{ij}\beta_j)^2\right\rbrace，
$$

$$
subject\quad to\quad\sum_{j=i}^p\beta^2_j\leq t,
$$

LASSO回归：
$$
\hat{β}^{ridge}=argmin\left\lbrace \sum_{i=1}^{N}(y_i-\beta_0-\sum_{j=1}^{p}x_{ij}\beta_j)^2\right\rbrace，
$$

$$
subject\quad to\quad\sum_{j=i}^p|\beta_j|\leq t,
$$

对比岭回归和LASSO回归，发现只有约束方程不同。

约束方程的更一般表现形式：
$$
\hat{β}=argmin\left\lbrace \sum_{i=1}^{N}(y_i-\beta_0-\sum_{j=1}^{p}x_{ij}\beta_j)^2+λ\sum_{j=1}^p|\beta_j|^p\right\rbrace，
$$
约束方程中的p可以有以下几种取值，每一种不同的取值对应一种回归算法(回归模型)

![](https://github.com/user-wbgithub/R/blob/master/images/2020-02-06_191835.jpg)

LASSO回归的几何表达：

![](https://github.com/user-wbgithub/R/blob/master/images/2020-02-06_191904.jpg)

目前采用LAR（最小角回归）作为LASSO等效的高效解法

#### LAR最小角回归算法

LAR既可以解决多重共线性，也可以对自变量进行筛选，同时也类似于逐步回归，但是性能比逐步回归要好。因为最小角回归采用的不是对自变量进行排列组合而是累加的方式来寻找最优模型的。

下面是具体细节：

![](https://github.com/user-wbgithub/R/blob/master/images/2020-02-06_203653.jpg)

​	逐步回归是迭代算法，会对各个自变量进行组合，自变量个数较多时会迭代次数过多，导致计算代价过大。逐步回归的指标是AIC，选择AIC最小的模型；

![](https://github.com/user-wbgithub/R/blob/master/images/2020-02-06_203705.jpg)

​	LAR会从和因变量比较相近的自变量开始计算，有多少个自变量就计算多少步。LAR的指标是CP，选择CP最小的指标；

LAR的几何解释：

![](https://github.com/user-wbgithub/R/blob/master/images/2020-02-06_203747.jpg)

LAR的建模：

​	安装LAR程序包-->加载lars包-->数据框数据集-->建模(lar(自变量矩阵，因变量矩阵，type=lar))-->查看模型(model)-->查看每一步CP值(summary())-->查看每项系数()

```R
> utils:::menuInstallLocal()
程序包‘lars’打开成功，MD5和检查也通过
> library(lars)
Loaded lars 1.2

> data<-read.table("cement.txt",header=T)
> data
   X1 X2 X3 X4     Y
1   7 26  6 60  78.5
2   1 29 15 52  74.3
3  11 56  8 20 104.3
4  11 31  8 47  87.6
5   7 52  6 33  95.9
6  11 55  9 22 109.2
7   3 71 17  6 102.7
8   1 31 22 44  72.5
9   2 54 18 22  93.1
10 21 47  4 26 115.9
11  1 40 23 34  83.8
12 11 66  9 12 113.3
13 10 68  8 12 109.4
> x<-as.matrix(data[,1:4])
> y<-as.matrix(data[,5])
> x
      X1 X2 X3 X4
      X1 X2 X3 X4
 [1,]  7 26  6 60
 [2,]  1 29 15 52
 [3,] 11 56  8 20
 [4,] 11 31  8 47
 [5,]  7 52  6 33
 [6,] 11 55  9 22
 [7,]  3 71 17  6
 [8,]  1 31 22 44
 [9,]  2 54 18 22
[10,] 21 47  4 26
[11,]  1 40 23 34
[12,] 11 66  9 12
[13,] 10 68  8 12
> y
       [,1]
 [1,]  78.5
 [2,]  74.3
 [3,] 104.3
 [4,]  87.6
 [5,]  95.9
 [6,] 109.2
 [7,] 102.7
 [8,]  72.5
 [9,]  93.1
[10,] 115.9
[11,]  83.8
[12,] 113.3
[13,] 109.4
> model.lar<-lars(x,y,type="lar")
> model.lar

Call:
lars(x = x, y = y, type = "lar")
R-squared: 0.982 
Sequence of LAR moves:
     X4 X1 X2 X3
Var   4  1  2  3
Step  1  2  3  4
> summary(model.lar)
LARS/LAR
Call: lars(x = x, y = y, type = "lar")
  Df     Rss       Cp
0  1 2715.76 442.9167
1  2 2219.35 361.9455
2  3 1917.55 313.5020
3  4   47.97   3.0184
4  5   47.86   5.0000
> coef(model.lar)
            X1        X2        X3         X4
[1,] 0.0000000 0.0000000 0.0000000  0.0000000
[2,] 0.0000000 0.0000000 0.0000000 -0.1079008
[3,] 0.1051605 0.0000000 0.0000000 -0.1448580
[4,] 1.4508509 0.4157739 0.0000000 -0.2364662
[5,] 1.5511026 0.5101676 0.1019094 -0.1440610
```

LAR模型的R方值很高，自变量依次验证的顺序是X4->X1->X2-X3，通过查看模型选择到CP值最低的自变量，剔除其余自变量，通过coef查看每个自变量对应的系数。如下图：

![](https://github.com/user-wbgithub/R/blob/master/images/2020-02-06_205754.jpg)

![](https://github.com/user-wbgithub/R/blob/master/images/2020-02-06_205809.jpg)