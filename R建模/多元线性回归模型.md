### 建立多元线性回归模型

**多元线性与一元线性相比自变量变为多个而不是一个，具体建模如下**：

建立数据集(数据框)-->建立模型(lm)-->检测模型(summary)-->使用模型做预测(predict)

```R
> blood<-data.frame(
+ X1=c(76.0, 91.5, 85.5, 82.5, 79.0, 80.5, 74.5,
+ 79.0, 85.0, 76.5, 82.0, 95.0, 92.5),
+ X2=c(50, 20, 20, 30, 30, 50, 60, 50, 40, 55,
+ 40, 40, 20),
+ Y= c(120, 141, 124, 126, 117, 125, 123, 125,132, 123, 132, 155, 147))

> lm.sol<-lm(Y ~ X1+X2, data=blood)

> blood
     X1 X2   Y
1  76.0 50 120
2  91.5 20 141
3  85.5 20 124
4  82.5 30 126
5  79.0 30 117
6  80.5 50 125
7  74.5 60 123
8  79.0 50 125
9  85.0 40 132
10 76.5 55 123
11 82.0 40 132
12 95.0 40 155
13 92.5 20 147
>

> summary(lm.sol) 

Call:
lm(formula = Y ~ X1 + X2, data = blood)

Residuals:
    Min      1Q  Median      3Q     Max 
-4.0404 -1.0183  0.4640  0.6908  4.3274 

Coefficients:
             Estimate Std. Error t value Pr(>|t|)    
(Intercept) -62.96336   16.99976  -3.704 0.004083 ** 
X1            2.13656    0.17534  12.185 2.53e-07 ***
X2            0.40022    0.08321   4.810 0.000713 ***
---
Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1

Residual standard error: 2.854 on 10 degrees of freedom
Multiple R-squared:  0.9461,    Adjusted R-squared:  0.9354 
F-statistic: 87.84 on 2 and 10 DF,  p-value: 4.531e-07

> new <- data.frame(X1 = 80, X2 = 40)
> lm.pred<-predict(lm.sol, new, interval="prediction", level=0.95)
> lm.pred
       fit      lwr      upr
1 123.9699 117.2889 130.6509
```



### 多重共线性

#### 	完全共线性

​		回归模型自变量之间存在精确相关关系或者高度相关关系，使得模型失真。	

```R
> toothpaste<-read.table("toothpaste.txt",header=T)
> toothpaste
     X1   X2    X3   X4    Y
1  3.85 3.80 -0.05 5.50 7.38
2  3.75 4.00  0.25 6.75 8.51
3  3.70 4.30  0.60 7.25 9.52
4  3.70 3.70  0.00 5.50 7.50
5  3.60 3.85  0.25 7.00 9.33
> model<-lm(Y~X1+X2+X3+X4,data=toothpaste)
>
> summary(model)

Call:
lm(formula = Y ~ X1 + X2 + X3 + X4, data = toothpaste)

Residuals:
     Min       1Q   Median       3Q      Max 
-0.41148 -0.11647 -0.01044  0.13584  0.51221 

Coefficients: (1 not defined because of singularities)
            Estimate Std. Error t value Pr(>|t|)    
(Intercept)   7.5227     2.4450   3.077 0.004881 ** 
X1           -2.3352     0.6388  -3.655 0.001141 ** 
X2            1.6006     0.2961   5.406 1.15e-05 ***
X3                NA         NA      NA       NA    
X4            0.5057     0.1261   4.009 0.000457 ***
---
Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1

Residual standard error: 0.2341 on 26 degrees of freedom
Multiple R-squared:  0.8942,    Adjusted R-squared:  0.882 
F-statistic: 73.22 on 3 and 26 DF,  p-value: 8.307e-13
```

​		*其中多元R方值很高，模型拟合很好。但是X3为NA。其中X3=X2-X1存在精确的函数关系，数据存在**完全共线性***

#### 	近似共线性

​		回归模型自变量之间存在一定程度上的共线性（多见）

#### 	共线性的解决办法

##### 		解决方法

###### 		1.手动解决解决

​			*手动剔除具有相关性的自变量*

```R
> model<-lm(Y~X1+X2+X4,data=toothpaste)
> summary(model)

Call:
lm(formula = Y ~ X1 + X2 + X4, data = toothpaste)

Residuals:
     Min       1Q   Median       3Q      Max 
-0.41148 -0.11647 -0.01044  0.13584  0.51221 

Coefficients:
            Estimate Std. Error t value Pr(>|t|)    
(Intercept)   7.5227     2.4450   3.077 0.004881 ** 
X1           -2.3352     0.6388  -3.655 0.001141 ** 
X2            1.6006     0.2961   5.406 1.15e-05 ***
X4            0.5057     0.1261   4.009 0.000457 ***
---
Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1

Residual standard error: 0.2341 on 26 degrees of freedom
Multiple R-squared:  0.8942,    Adjusted R-squared:  0.882 
F-statistic: 73.22 on 3 and 26 DF,  p-value: 8.307e-13
```

​			*手动剔除共线性自变量当自变量很多时，会尝试很多次，所以不适用于生产*。

###### 		2.逐步回归方法解决

```
> mode<-lm(Y~X1+X2+X3+X4,data=data)
> summary(model)

Call:
lm(formula = Y ~ X1 + X2 + X3 + X4, data = data)

Residuals:
    Min      1Q  Median      3Q     Max 
-3.1750 -1.6709  0.2508  1.3783  3.9254 

Coefficients:
            Estimate Std. Error t value Pr(>|t|)  
(Intercept)  62.4054    70.0710   0.891   0.3991  
X1            1.5511     0.7448   2.083   0.0708 .
X2            0.5102     0.7238   0.705   0.5009  
X3            0.1019     0.7547   0.135   0.8959  
X4           -0.1441     0.7091  -0.203   0.8441  
---
Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1

Residual standard error: 2.446 on 8 degrees of freedom
Multiple R-squared:  0.9824,    Adjusted R-squared:  0.9736 
F-statistic: 111.5 on 4 and 8 DF,  p-value: 4.756e-07
```

​			*①这里R方值很高，模型拟合度很好，但是所有的系数都没有通过检验。Pr都大于0.05。这里通过逐步回归方法剔除变量。*

​			*②逐步回归方法step()评估指标是AIC，AIC越小说明效果越好。底层使每次剔除一个自变量*。

```R
> model<-lm(Y~X1+X2+X3+X4,data=data)
> model.step<-step(model)
Start:  AIC=26.94
Y ~ X1 + X2 + X3 + X4

       Df Sum of Sq    RSS    AIC
- X3    1    0.1091 47.973 24.974
- X4    1    0.2470 48.111 25.011
- X2    1    2.9725 50.836 25.728
<none>              47.864 26.944
- X1    1   25.9509 73.815 30.576

Step:  AIC=24.97
Y ~ X1 + X2 + X4

       Df Sum of Sq    RSS    AIC
<none>               47.97 24.974
- X4    1      9.93  57.90 25.420
- X2    1     26.79  74.76 28.742
- X1    1    820.91 868.88 60.629
```

​			*①模型调用step()进行逐步回归，根据AIC从大到小进行剔除变量，最小的效果最好。上面代码第一次去除X3后效果最好，第二次什么都不剔除效果最好。查看剔除后的模型：*

```R
> summary(model.step)

Call:
lm(formula = Y ~ X1 + X2 + X4, data = data)

Residuals:
    Min      1Q  Median      3Q     Max 
-3.0919 -1.8016  0.2562  1.2818  3.8982 

Coefficients:
            Estimate Std. Error t value Pr(>|t|)    
(Intercept)  71.6483    14.1424   5.066 0.000675 ***
X1            1.4519     0.1170  12.410 5.78e-07 ***
X2            0.4161     0.1856   2.242 0.051687 .  
X4           -0.2365     0.1733  -1.365 0.205395    
---
Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1

Residual standard error: 2.309 on 9 degrees of freedom
Multiple R-squared:  0.9823,    Adjusted R-squared:  0.9764 
F-statistic: 166.8 on 3 and 9 DF,  p-value: 3.323e-08
```

​			*①.模型拟合度很高，但是有两个系数没有通过检验。通过逐步回归方法的辅助，选择AIC最小的X4进行进一步剔除。查看模型：*

```

> model<-lm(Y~X1+X2,data=data)
> summary(model)

Call:
lm(formula = Y ~ X1 + X2, data = data)

Residuals:
   Min     1Q Median     3Q    Max 
-2.893 -1.574 -1.302  1.363  4.048 

Coefficients:
            Estimate Std. Error t value Pr(>|t|)    
(Intercept) 52.57735    2.28617   23.00 5.46e-10 ***
X1           1.46831    0.12130   12.11 2.69e-07 ***
X2           0.66225    0.04585   14.44 5.03e-08 ***
---
Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1

Residual standard error: 2.406 on 10 degrees of freedom
Multiple R-squared:  0.9787,    Adjusted R-squared:  0.9744 
F-statistic: 229.5 on 2 and 10 DF,  p-value: 4.407e-09
```

​			*①.这里模型R方值很高，模型拟合很好，所有系数也都通过检验*

​			*②.以上就是使用逐步回归方法进行模型的优化，适用于生产*

###### 		3.岭回归算法解决

​			先导入岭回归的程序包-->导入ridge包-->数据框数据集-->建立回归模型(linearRidge)

```R
> utils:::menuInstallLocal()
程序包‘ridge’打开成功，MD5和检查也通过
> library(ridge)
> data<-read.table("cement.txt",header=T)
> data
   X1 X2 X3 X4     Y
1   7 26  6 60  78.5
2   1 29 15 52  74.3
3  11 56  8 20 104.3
4  11 31  8 47  87.6
5   7 52  6 33  95.9
6  11 55  9 22 109.2
7   3 71 17  6 102.7
8   1 31 22 44  72.5
9   2 54 18 22  93.1
10 21 47  4 26 115.9
11  1 40 23 34  83.8
12 11 66  9 12 113.3
13 10 68  8 12 109.4
> model<-lm(Y~X1+X2+X3+X4,data=data)
> summary(model)

Call:
lm(formula = Y ~ X1 + X2 + X3 + X4, data = data)

Residuals:
    Min      1Q  Median      3Q     Max 
-3.1750 -1.6709  0.2508  1.3783  3.9254 

Coefficients:
            Estimate Std. Error t value Pr(>|t|)  
(Intercept)  62.4054    70.0710   0.891   0.3991  
X1            1.5511     0.7448   2.083   0.0708 .
X2            0.5102     0.7238   0.705   0.5009  
X3            0.1019     0.7547   0.135   0.8959  
X4           -0.1441     0.7091  -0.203   0.8441  
---
Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1

Residual standard error: 2.446 on 8 degrees of freedom
Multiple R-squared:  0.9824,    Adjusted R-squared:  0.9736 
F-statistic: 111.5 on 4 and 8 DF,  p-value: 4.756e-07
```

​			这里使用最小二乘法建立的多元线性模型的拟合度很高，但是系数都没有通过检验，我们再建	立逻辑回归模型。

```R
> model.ridge<-linearRidge(Y~X1+X2+X3+X4,data=data)
> summary(model.ridge)

Call:
linearRidge(formula = Y ~ X1 + X2 + X3 + X4, data = data)


Coefficients:
            Estimate Scaled estimate Std. Error (scaled) t value (scaled)
(Intercept)  83.7040              NA                  NA               NA
X1            1.2922         26.3321              3.6721            7.171
X2            0.2977         16.0463              3.9883            4.023
X3           -0.1478         -3.2790              3.5979            0.911
X4           -0.3506        -20.3290              3.9963            5.087
            Pr(>|t|)    
(Intercept)       NA    
X1          7.45e-13 ***
X2          5.74e-05 ***
X3             0.362    
X4          3.64e-07 ***
---
Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1

Ridge parameter: 0.01473162, chosen automatically, computed using 2 PCs

Degrees of freedom: model 3.01 , variance 2.837 , residual 3.183 
```

​			使用岭回归算法之后X1、X2、X4系数都通过检验，但是X3系数未通过检验，岭回归模型并未对X3自变量进行剔除，这是岭回归的一个缺点，很难剔除共线性的列，达到筛选的效果。原因如下图：

![](https://github.com/user-wbgithub/R/blob/master/images/2020-02-06_160935.jpg)

​			假如要剔除β1必须是：最小二乘解和约束方程必须相交于β2轴上才可以，这非常难，所以岭回归算法很难剔除共线性列

### 算法

#### 	最小二乘法

​		最小二乘法有个使用条件：样本矩阵必须是满秩的。(数据集不能出现完全共线性的情况)

​		满秩---矩阵的所有行/列都不为0；

​		转秩---行变列裂变行T

##### 	多重共线性和最小二乘法

​		最小二乘法的限制：**样本矩阵的行列式不能为0**，样本矩阵必须是非奇异矩阵（满秩）。样本矩阵		行列式不为零，样本特征表现为：**个个特征之间不能表现为强烈线性相关性（多重共线性）**，线		性相关性经过初等变化会出现某行/列为0。

​		*假设y是因变量矩阵；β是系数矩阵；X是样本矩阵；ε是误差矩阵；*
$$
y = \left[
\matrix{
  y_1 & \\
  y_2 & \\
  ⋮ & \\
  y_3 
}
\right],β=\left[
\matrix{
  β_1 & \\
  β_2 & \\
  ⋮ & \\
  β_3 
}
\right],X=\left[
\matrix{
  1 & x_{11} &x_{12} &...&x_{1p}\\
  1 & x_{21} &x_{22} &...&x_{2p}\\
  ⋮ & ⋮&⋮&&⋮\\
  1 & x_{n1} & x_{2n} & ... & x_{np}
}
\right],ε=\left[
\matrix{
  ε_1 & \\
  ε_2 & \\
  ⋮ & \\
  ε_3 
}
\right]
$$
​		*多元线性模型可以表示为：*

​			
$$
Y=Xβ+ε
$$
​		*经过推导求得的系数最小二乘解可表示为如下形式：*

​	
$$
β=(X^TX)^{-1}X^TY
$$
​		*系数矩阵等于：X矩阵的转秩乘以X矩阵还是X矩阵，再求出这个整体的逆矩阵，再乘以X矩阵的转		秩和Y因变量矩阵。*

​		*当出现多重共线性时矩阵中某行/列为0：*
$$
A = \left[\matrix{1&2\\ 2&4\\ 3&6}\right]=\left[\matrix{2&2\\ 4&4\\ 6&6}\right]=\left[\matrix{0&2\\ 0&4\\ 0&6}\right]
$$

​		*计算A矩阵的转秩和A相乘*
$$
\left[\matrix{A^TA\\ }\right]=\left[\matrix{ 0&0&0\\ 2&4&6\\ }\right]=\left[\matrix{ 0&0\\ 0&56\\ } \right] =0\ast56-0\ast0=0
$$
​		*①此时计算出来的行列式矩阵是0，这个矩阵的逆所以当样本矩阵中存在多重共线性（完全）问题时，系数矩阵是求不出来的。所以最小二乘法不适用于样本矩阵不满秩的情况。*

​		*②当样本矩阵满秩，但是具有强烈**近似共线性**时，所求出来的系数矩阵很大，d模型失去解释能力；**最小二乘法适用场景：当样本矩阵的共线性程度较弱时，可以使用。***

​	*③强烈近似共线时，可以使用岭回归进行有偏估计，最小二乘法是一种无偏差估计。*

#### 岭回归算法

​		岭回归是**用于共线性数据分析的有偏估计回归方法，**在最小二乘法上进行了改良--**增加了惩戒系数**。放弃了最小二乘法的无偏性，以损失精度的方式获得回归系数更符合实际。对于病态数据的拟合要更优于最小二乘法。

​		最小二乘法：
$$
β=(X^TX)^{-1}X^TY
$$
​		岭回归：
$$
β=(X^TX+λI)^{-1}X^TY
$$
​		λ：岭参数大于0；Ⅰ：与样本矩阵同维度的单位矩阵；

​		①岭回归在最小二乘解上增加了一个小的扰动（惩罚措施），整体上要比最小二乘的估计值更稳定（在样本矩阵有严重共线性时）；

​		②岭回归是有偏估计的原因：在原数据的基础上引入了其他数据；

​		岭回归核心思想：寻找最岭参数λ，在这个零参数之下，使得残差平方和最小，从而求解出岭回归系数。
$$
β=(X^TX+λI)^{-1}X^TY
$$

$$
RSS(λ)=(y-Xβ)^T(y-Xβ)+λβ^Tβ\tag1
$$

​		(1)式等价于在λ作用下，求解系数β，使得RSS最小:
$$
\hat{β}^{ridge}=argmin\left\lbrace \sum_{i=1}^{N}(y_i-\beta_0-\sum_{j=1}^{p}x_{ij}\beta_j)^2+\lambda\sum_{j=1}^p\beta^2_j\right\rbrace\tag2
$$
​		进一步将(2)式推导，转变为就系数βj的约束边界问题:
$$
\hat{β}^{ridge}=argmin\left\lbrace \sum_{i=1}^{N}(y_i-\beta_0-\sum_{j=1}^{p}x_{ij}\beta_j)^2\right\rbrace，
$$

$$
subject\quad to\quad\sum_{j=i}^p\beta^2_j\leq t,\tag3
$$

​		边界约束的解释：

​		例如有两个自变量身高X1和体重X2，因变量Y式某种激素水平，建立的模型为：
$$
Y=\beta0+\beta1X1+\beta2X2
$$
​		在公式(3)中的约束为：
$$
\beta1^2+\beta2^2 ≦t
$$
​		①直接使用最小二乘法去拟合得到唯一解，但是X1和X2严重相关，可能会出现β1和β2相互抵消的效应，如β1是一个很大的数，同时β2是一个绝对值很大负数。虽然使得Y的变换很稳定，但是模型已经失真。

​		②转变为使用岭回归法加上一个约束，将β1和β2固定在一个区间之内。

#### 岭回归算法的几何解释

​		以上面边界约束中的例子为例，求解一组β1和β2，使得RSS最小，并受限于约束公式为：
$$
\beta1^2+\beta2^2 ≦t
$$
​		求解两个系数为：

![。](https://github.com/user-wbgithub/R/blob/master/images/2020-02-06_144210.jpg)

​		凸图形最低点时最小二乘解，岭回归解是：约束方程和最小二乘解的焦点处对应的坐标解。具体可以用拉格朗日定理求解，解决有参数化的问题。

#### 岭际图

​		岭际图描述的是零参数k变化时，系数值的变化。岭际图呈喇叭口形状，则可以认为数据存在对冲共线性，张口越大共线性越强

![1](https://github.com/user-wbgithub/R/blob/master/images/1.png)

​		λ等于0时，系数值较大，但当λ增大系数值迅速缩小，说明引入零参数之后有很强的限制效果，说明有严重的共线性

![2](https://github.com/user-wbgithub/R/blob/master/images/2.png)

​		这个岭际图也反映出多个变量之间存在共线性问题

![3](https://github.com/user-wbgithub/R/blob/master/images/3.png)

​		这个岭际图比较平稳，最小二乘法可以有更大信心



